{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmp</th>\n",
       "      <th>hum</th>\n",
       "      <th>snr</th>\n",
       "      <th>CO2</th>\n",
       "      <th>VOC</th>\n",
       "      <th>vis</th>\n",
       "      <th>IR</th>\n",
       "      <th>WIFI</th>\n",
       "      <th>BLE</th>\n",
       "      <th>rssi</th>\n",
       "      <th>...</th>\n",
       "      <th>device_id_hka-aqm-am201a</th>\n",
       "      <th>device_id_hka-aqm-am201b</th>\n",
       "      <th>device_id_hka-aqm-am204</th>\n",
       "      <th>device_id_hka-aqm-am205</th>\n",
       "      <th>device_id_hka-aqm-am209</th>\n",
       "      <th>device_id_hka-aqm-am210</th>\n",
       "      <th>device_id_hka-aqm-am211</th>\n",
       "      <th>device_id_hka-aqm-am301</th>\n",
       "      <th>device_id_hka-aqm-am307</th>\n",
       "      <th>device_id_hka-aqm-am308</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46711</th>\n",
       "      <td>25.080000</td>\n",
       "      <td>44.9700</td>\n",
       "      <td>-16.800000</td>\n",
       "      <td>754</td>\n",
       "      <td>558</td>\n",
       "      <td>379</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-131</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54194</th>\n",
       "      <td>23.900000</td>\n",
       "      <td>52.1100</td>\n",
       "      <td>-15.200000</td>\n",
       "      <td>686</td>\n",
       "      <td>593</td>\n",
       "      <td>255</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-135</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54195</th>\n",
       "      <td>24.137500</td>\n",
       "      <td>51.8300</td>\n",
       "      <td>-11.625000</td>\n",
       "      <td>800</td>\n",
       "      <td>633</td>\n",
       "      <td>256</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54196</th>\n",
       "      <td>24.432500</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>-15.650000</td>\n",
       "      <td>902</td>\n",
       "      <td>825</td>\n",
       "      <td>289</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24671</th>\n",
       "      <td>24.908000</td>\n",
       "      <td>52.3160</td>\n",
       "      <td>-2.550000</td>\n",
       "      <td>1128</td>\n",
       "      <td>450</td>\n",
       "      <td>213</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-115</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24669</th>\n",
       "      <td>24.595000</td>\n",
       "      <td>45.5850</td>\n",
       "      <td>-14.900000</td>\n",
       "      <td>469</td>\n",
       "      <td>790</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84148</th>\n",
       "      <td>27.086667</td>\n",
       "      <td>38.9300</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>441</td>\n",
       "      <td>836</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-102</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61166</th>\n",
       "      <td>24.035000</td>\n",
       "      <td>48.9550</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10026</td>\n",
       "      <td>9997</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-96</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>24.492500</td>\n",
       "      <td>45.9275</td>\n",
       "      <td>-9.700000</td>\n",
       "      <td>445</td>\n",
       "      <td>865</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-120</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54192</th>\n",
       "      <td>22.990000</td>\n",
       "      <td>51.6250</td>\n",
       "      <td>-7.575000</td>\n",
       "      <td>454</td>\n",
       "      <td>810</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>-124</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143802 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tmp      hum        snr    CO2   VOC  vis  IR  WIFI  BLE  rssi  \\\n",
       "46711  25.080000  44.9700 -16.800000    754   558  379  64     4    0  -131   \n",
       "54194  23.900000  52.1100 -15.200000    686   593  255  35     5    0  -135   \n",
       "54195  24.137500  51.8300 -11.625000    800   633  256  36     3    0  -125   \n",
       "54196  24.432500  52.0000 -15.650000    902   825  289  55     5    2  -125   \n",
       "24671  24.908000  52.3160  -2.550000   1128   450  213  91     3    1  -115   \n",
       "...          ...      ...        ...    ...   ...  ...  ..   ...  ...   ...   \n",
       "24669  24.595000  45.5850 -14.900000    469   790    5   1     2    1  -125   \n",
       "84148  27.086667  38.9300   9.066667    441   836   32   4     5    0  -102   \n",
       "61166  24.035000  48.9550   6.000000  10026  9997   12   3     2    2   -96   \n",
       "7473   24.492500  45.9275  -9.700000    445   865    8   2     1    4  -120   \n",
       "54192  22.990000  51.6250  -7.575000    454   810    7   2     3   17  -124   \n",
       "\n",
       "       ...  device_id_hka-aqm-am201a  device_id_hka-aqm-am201b  \\\n",
       "46711  ...                     False                     False   \n",
       "54194  ...                     False                     False   \n",
       "54195  ...                     False                     False   \n",
       "54196  ...                     False                     False   \n",
       "24671  ...                     False                     False   \n",
       "...    ...                       ...                       ...   \n",
       "24669  ...                     False                     False   \n",
       "84148  ...                     False                     False   \n",
       "61166  ...                     False                     False   \n",
       "7473   ...                     False                     False   \n",
       "54192  ...                     False                     False   \n",
       "\n",
       "       device_id_hka-aqm-am204  device_id_hka-aqm-am205  \\\n",
       "46711                    False                    False   \n",
       "54194                    False                    False   \n",
       "54195                    False                    False   \n",
       "54196                    False                    False   \n",
       "24671                    False                    False   \n",
       "...                        ...                      ...   \n",
       "24669                    False                    False   \n",
       "84148                    False                    False   \n",
       "61166                    False                    False   \n",
       "7473                     False                    False   \n",
       "54192                    False                    False   \n",
       "\n",
       "       device_id_hka-aqm-am209  device_id_hka-aqm-am210  \\\n",
       "46711                    False                    False   \n",
       "54194                    False                    False   \n",
       "54195                    False                    False   \n",
       "54196                    False                    False   \n",
       "24671                    False                    False   \n",
       "...                        ...                      ...   \n",
       "24669                    False                    False   \n",
       "84148                    False                    False   \n",
       "61166                    False                    False   \n",
       "7473                     False                    False   \n",
       "54192                    False                    False   \n",
       "\n",
       "       device_id_hka-aqm-am211  device_id_hka-aqm-am301  \\\n",
       "46711                    False                    False   \n",
       "54194                    False                    False   \n",
       "54195                    False                    False   \n",
       "54196                    False                    False   \n",
       "24671                    False                    False   \n",
       "...                        ...                      ...   \n",
       "24669                    False                    False   \n",
       "84148                    False                    False   \n",
       "61166                    False                    False   \n",
       "7473                     False                    False   \n",
       "54192                    False                    False   \n",
       "\n",
       "       device_id_hka-aqm-am307  device_id_hka-aqm-am308  \n",
       "46711                    False                    False  \n",
       "54194                    False                    False  \n",
       "54195                    False                    False  \n",
       "54196                    False                    False  \n",
       "24671                    False                    False  \n",
       "...                        ...                      ...  \n",
       "24669                    False                    False  \n",
       "84148                    False                    False  \n",
       "61166                    False                    False  \n",
       "7473                     False                    False  \n",
       "54192                    False                    False  \n",
       "\n",
       "[143802 rows x 59 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('aggregated_hourly.csv')\n",
    "#for every distinct device_id create a new column target with the value of WIFI shiftied by 1\n",
    "df['target'] = 0\n",
    "for i in df['device_id'].unique():\n",
    "    df.loc[df['device_id'] == i, 'target'] = df.loc[df['device_id'] == i, 'CO2'].shift(-1)\n",
    "df.sort_values(by=['date_time'], inplace=True)\n",
    "#drop\n",
    "df = df.dropna()   \n",
    "#onehot encode device_id into int\n",
    "df = pd.get_dummies(df, columns=['device_id'], prefix = 'device_id')\n",
    "#delete date_time column and device_id column\n",
    "df = df.drop(columns=['date_time'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([143802, 58])\n",
      "torch.Size([143802])\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['target']).astype('float32')\n",
    "y = df['target'].astype('float32')\n",
    "X = torch.tensor(X.values).float()\n",
    "y = torch.tensor(y.values).float()\n",
    "print(X.shape)  \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143801, 500, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = []\n",
    "window_size = 500\n",
    "#for i in range(0, len(X), window_size):\n",
    "for i in range(0, len(X)):\n",
    "    if i < window_size:\n",
    "        number_of_padding = window_size - i\n",
    "        padding = torch.zeros(number_of_padding, X.shape[1])\n",
    "        X_new.append(torch.cat((padding, X[:i])))\n",
    "    else:\n",
    "        X_new.append(X[i-window_size:i])\n",
    "X_new = torch.stack(X_new)\n",
    "X_new = X_new[1:]\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "batch_size=6\n",
    "# Create a TensorDataset\n",
    "y = y[1:]\n",
    "data = TensorDataset(X_new, y)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size= 1, hidden_layer_size= 26, num_layers= 2, output_size= 1, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.linear1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size= self.hidden_layer_size, num_layers=num_layers, dropout=dropout, batch_first= True)\n",
    "        self.Linear2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        hn.permute(1, 0, 2).reshape(batch_size, -1)\n",
    "\n",
    "        predictions = self.Linear2(hn)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([1, 5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 187154.4375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, labels)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = LSTM(input_size=X_new.shape[2], hidden_layer_size=26, output_size=1, num_layers=1, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for inputs, labels in data_loader:\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
