{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmp</th>\n",
       "      <th>hum</th>\n",
       "      <th>snr</th>\n",
       "      <th>CO2</th>\n",
       "      <th>VOC</th>\n",
       "      <th>vis</th>\n",
       "      <th>IR</th>\n",
       "      <th>WIFI</th>\n",
       "      <th>BLE</th>\n",
       "      <th>rssi</th>\n",
       "      <th>...</th>\n",
       "      <th>device_id_hka-aqm-am201a</th>\n",
       "      <th>device_id_hka-aqm-am201b</th>\n",
       "      <th>device_id_hka-aqm-am204</th>\n",
       "      <th>device_id_hka-aqm-am205</th>\n",
       "      <th>device_id_hka-aqm-am209</th>\n",
       "      <th>device_id_hka-aqm-am210</th>\n",
       "      <th>device_id_hka-aqm-am211</th>\n",
       "      <th>device_id_hka-aqm-am301</th>\n",
       "      <th>device_id_hka-aqm-am307</th>\n",
       "      <th>device_id_hka-aqm-am308</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46711</th>\n",
       "      <td>25.080000</td>\n",
       "      <td>44.9700</td>\n",
       "      <td>-16.800000</td>\n",
       "      <td>754</td>\n",
       "      <td>558</td>\n",
       "      <td>379</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-131</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54194</th>\n",
       "      <td>23.900000</td>\n",
       "      <td>52.1100</td>\n",
       "      <td>-15.200000</td>\n",
       "      <td>686</td>\n",
       "      <td>593</td>\n",
       "      <td>255</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-135</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54195</th>\n",
       "      <td>24.137500</td>\n",
       "      <td>51.8300</td>\n",
       "      <td>-11.625000</td>\n",
       "      <td>800</td>\n",
       "      <td>633</td>\n",
       "      <td>256</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54196</th>\n",
       "      <td>24.432500</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>-15.650000</td>\n",
       "      <td>902</td>\n",
       "      <td>825</td>\n",
       "      <td>289</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24671</th>\n",
       "      <td>24.908000</td>\n",
       "      <td>52.3160</td>\n",
       "      <td>-2.550000</td>\n",
       "      <td>1128</td>\n",
       "      <td>450</td>\n",
       "      <td>213</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-115</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24669</th>\n",
       "      <td>24.595000</td>\n",
       "      <td>45.5850</td>\n",
       "      <td>-14.900000</td>\n",
       "      <td>469</td>\n",
       "      <td>790</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-125</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84148</th>\n",
       "      <td>27.086667</td>\n",
       "      <td>38.9300</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>441</td>\n",
       "      <td>836</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-102</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61166</th>\n",
       "      <td>24.035000</td>\n",
       "      <td>48.9550</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10026</td>\n",
       "      <td>9997</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-96</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>24.492500</td>\n",
       "      <td>45.9275</td>\n",
       "      <td>-9.700000</td>\n",
       "      <td>445</td>\n",
       "      <td>865</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-120</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54192</th>\n",
       "      <td>22.990000</td>\n",
       "      <td>51.6250</td>\n",
       "      <td>-7.575000</td>\n",
       "      <td>454</td>\n",
       "      <td>810</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>-124</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143802 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tmp      hum        snr    CO2   VOC  vis  IR  WIFI  BLE  rssi  \\\n",
       "46711  25.080000  44.9700 -16.800000    754   558  379  64     4    0  -131   \n",
       "54194  23.900000  52.1100 -15.200000    686   593  255  35     5    0  -135   \n",
       "54195  24.137500  51.8300 -11.625000    800   633  256  36     3    0  -125   \n",
       "54196  24.432500  52.0000 -15.650000    902   825  289  55     5    2  -125   \n",
       "24671  24.908000  52.3160  -2.550000   1128   450  213  91     3    1  -115   \n",
       "...          ...      ...        ...    ...   ...  ...  ..   ...  ...   ...   \n",
       "24669  24.595000  45.5850 -14.900000    469   790    5   1     2    1  -125   \n",
       "84148  27.086667  38.9300   9.066667    441   836   32   4     5    0  -102   \n",
       "61166  24.035000  48.9550   6.000000  10026  9997   12   3     2    2   -96   \n",
       "7473   24.492500  45.9275  -9.700000    445   865    8   2     1    4  -120   \n",
       "54192  22.990000  51.6250  -7.575000    454   810    7   2     3   17  -124   \n",
       "\n",
       "       ...  device_id_hka-aqm-am201a  device_id_hka-aqm-am201b  \\\n",
       "46711  ...                     False                     False   \n",
       "54194  ...                     False                     False   \n",
       "54195  ...                     False                     False   \n",
       "54196  ...                     False                     False   \n",
       "24671  ...                     False                     False   \n",
       "...    ...                       ...                       ...   \n",
       "24669  ...                     False                     False   \n",
       "84148  ...                     False                     False   \n",
       "61166  ...                     False                     False   \n",
       "7473   ...                     False                     False   \n",
       "54192  ...                     False                     False   \n",
       "\n",
       "       device_id_hka-aqm-am204  device_id_hka-aqm-am205  \\\n",
       "46711                    False                    False   \n",
       "54194                    False                    False   \n",
       "54195                    False                    False   \n",
       "54196                    False                    False   \n",
       "24671                    False                    False   \n",
       "...                        ...                      ...   \n",
       "24669                    False                    False   \n",
       "84148                    False                    False   \n",
       "61166                    False                    False   \n",
       "7473                     False                    False   \n",
       "54192                    False                    False   \n",
       "\n",
       "       device_id_hka-aqm-am209  device_id_hka-aqm-am210  \\\n",
       "46711                    False                    False   \n",
       "54194                    False                    False   \n",
       "54195                    False                    False   \n",
       "54196                    False                    False   \n",
       "24671                    False                    False   \n",
       "...                        ...                      ...   \n",
       "24669                    False                    False   \n",
       "84148                    False                    False   \n",
       "61166                    False                    False   \n",
       "7473                     False                    False   \n",
       "54192                    False                    False   \n",
       "\n",
       "       device_id_hka-aqm-am211  device_id_hka-aqm-am301  \\\n",
       "46711                    False                    False   \n",
       "54194                    False                    False   \n",
       "54195                    False                    False   \n",
       "54196                    False                    False   \n",
       "24671                    False                    False   \n",
       "...                        ...                      ...   \n",
       "24669                    False                    False   \n",
       "84148                    False                    False   \n",
       "61166                    False                    False   \n",
       "7473                     False                    False   \n",
       "54192                    False                    False   \n",
       "\n",
       "       device_id_hka-aqm-am307  device_id_hka-aqm-am308  \n",
       "46711                    False                    False  \n",
       "54194                    False                    False  \n",
       "54195                    False                    False  \n",
       "54196                    False                    False  \n",
       "24671                    False                    False  \n",
       "...                        ...                      ...  \n",
       "24669                    False                    False  \n",
       "84148                    False                    False  \n",
       "61166                    False                    False  \n",
       "7473                     False                    False  \n",
       "54192                    False                    False  \n",
       "\n",
       "[143802 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('aggregated_hourly.csv')\n",
    "#for every distinct device_id create a new column target with the value of WIFI shiftied by 1\n",
    "df['target'] = 0\n",
    "for i in df['device_id'].unique():\n",
    "    df.loc[df['device_id'] == i, 'target'] = df.loc[df['device_id'] == i, 'WIFI'].shift(-1)\n",
    "df.sort_values(by=['date_time'], inplace=True)\n",
    "#drop\n",
    "df = df.dropna()   \n",
    "#onehot encode device_id into int\n",
    "df = pd.get_dummies(df, columns=['device_id'], prefix = 'device_id')\n",
    "#delete date_time column and device_id column\n",
    "df = df.drop(columns=['date_time'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([143802, 58])\n",
      "torch.Size([143802])\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['target']).astype('float32')\n",
    "y = df['target'].astype('float32')\n",
    "X = torch.tensor(X.values).float()\n",
    "y = torch.tensor(y.values).float()\n",
    "print(X.shape)  \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143801, 500, 58])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = []\n",
    "window_size = 500\n",
    "#for i in range(0, len(X), window_size):\n",
    "for i in range(0, len(X)):\n",
    "    if i < window_size:\n",
    "        number_of_padding = window_size - i\n",
    "        padding = torch.zeros(number_of_padding, X.shape[1])\n",
    "        X_new.append(torch.cat((padding, X[:i])))\n",
    "    else:\n",
    "        X_new.append(X[i-window_size:i])\n",
    "X_new = torch.stack(X_new)\n",
    "X_new = X_new[1:]\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143801, 500, 58])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x28c684247f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a TensorDataset\n",
    "y = y[1:]\n",
    "data = TensorDataset(X_new, y)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    compute sinusoid encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len,device):\n",
    "        \"\"\"\n",
    "        constructor of sinusoid encoding class\n",
    "\n",
    "        :param d_model: dimension of model\n",
    "        :param max_len: max sequence length\n",
    "        :param device: hardware device setting\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # same size with input matrix (for adding with input matrix)\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
    "        pos = torch.arange(0, max_len,device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        # # 1D => 2D unsqueeze to represent word's position\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
    "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        # compute positional encoding to consider positional information of words\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.encoding\n",
    "        # [max_len = 512, d_model = 512]\n",
    "\n",
    "        #batch_size, seq_len = x.size()\n",
    "        # [batch_size = 128, seq_len = 30]\n",
    "\n",
    "        return self.encoding\n",
    "        #return self.encoding[:seq_len, :]\n",
    "        # [seq_len = 30, d_model = 512]\n",
    "        # it will add with tok_emb : [128, 30, 512]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    scaled dot product attention class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        constructor of scaled dot product attention class\n",
    "        \"\"\"\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        forward pass of scaled dot product attention\n",
    "        :param Q: query tensor\n",
    "        :param K: key tensor\n",
    "        :param V: value tensor\n",
    "        :param mask: mask tensor\n",
    "        :return: output tensor\n",
    "        \"\"\"\n",
    "        d_k = K.size(-1)\n",
    "        # get dimension of key\n",
    "        scores = (Q @ K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        # compute attention score\n",
    "        # if mask is not None:\n",
    "        #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        #     # apply mask to score\n",
    "        attention = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        # apply softmax to score\n",
    "        output = attention @ V\n",
    "        # compute output tensor\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    multihead attention class\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        constructor of multihead attention class\n",
    "\n",
    "        :param d_model: dimension of model\n",
    "        :param num_heads: number of head in multihead attention\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        # get dimension of key\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_model)\n",
    "        # linear transformation for query, key, value\n",
    "\n",
    "        self.scaled_dot_product = ScaledDotProduct()\n",
    "        # scaled dot product attention\n",
    "\n",
    "        self.linear = torch.nn.Linear(d_model, d_model)\n",
    "        # linear transformation for output\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        forward pass of multihead attention\n",
    "\n",
    "        :param Q: query tensor\n",
    "        :param K: key tensor\n",
    "        :param V: value tensor\n",
    "        :param mask: mask tensor\n",
    "        :return: output tensor\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        # get batch size\n",
    "\n",
    "        Q = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # linear transformation and split into multihead\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            # unsqueeze mask\n",
    "\n",
    "        output, attention = self.scaled_dot_product(Q, K, V, mask)\n",
    "        # scaled dot product attention\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        # concatenate multihead attention\n",
    "\n",
    "        return self.linear(output), attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    decoder layer class\n",
    "    \"\"\"\n",
    "    def __init__(self, input,d_model,max_len,num_heads,d_ff,device):\n",
    "        \"\"\"\n",
    "        constructor of decoder layer\n",
    "\n",
    "        :param d_model: dimension of model\n",
    "        :param num_heads: number of head in multihead attention\n",
    "        :param d_ff: dimension of feed forward layer\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embed = torch.nn.Linear(input, d_model).to(device)\n",
    "        self.positonal_encoding = PositionalEncoding(d_model, max_len=max_len,device=device).to(device)\n",
    "        self.norm = torch.nn.LayerNorm(d_model).to(device)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads).to(device)\n",
    "\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_ff, 1)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward pass of decoder layer\n",
    "\n",
    "        :param x: input tensor (query)\n",
    "        :param memory: input tensor (key, value)\n",
    "        :param src_mask: source mask\n",
    "        :param tgt_mask: target mask\n",
    "        :return: output tensor\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        x = x + self.positonal_encoding(x)\n",
    "        x_norm = self.norm(x)\n",
    "        x_att, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + x_att\n",
    "        x = self.ff(x) #shape am ende noch (500,1), soll des so ?\n",
    "        return x[:, -1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Decoder1 = Decoder(data[0][0].shape[1], 32,max_len=data[0][0].shape[0],num_heads=4,d_ff=128,device=device)\n",
    "d = Decoder1.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 500, 58])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decoder\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Create a DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Create a Decoder\n",
    "Decoder1 = Decoder(data[0][0].shape[1], 32,max_len=data[0][0].shape[0],num_heads=2,d_ff=64,device=device)\n",
    "\n",
    "# Create a Loss Function\n",
    "criterion = MSELoss()\n",
    "\n",
    "# Create an Optimizer\n",
    "optimizer = Adam(Decoder1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3.8604\n",
      "Epoch: 002, Loss: 1.5877\n",
      "Epoch: 003, Loss: 1.9125\n",
      "Epoch: 004, Loss: 1.6431\n",
      "Epoch: 005, Loss: 2.4121\n",
      "Epoch: 006, Loss: 4.3164\n",
      "Epoch: 007, Loss: 4.3205\n",
      "Epoch: 008, Loss: 1.9703\n",
      "Epoch: 009, Loss: 1.5076\n",
      "Epoch: 010, Loss: 5.3296\n"
     ]
    }
   ],
   "source": [
    "# Train the Decoder\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Move model to GPU\n",
    "Decoder1 = Decoder1.to(device)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Decoder1.parameters(), lr=0.001)\n",
    "\n",
    "Decoder1.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in data_loader:\n",
    "        # Move tensors to GPU\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "        # break\n",
    "\n",
    "        # Forward pass\n",
    "        output = Decoder1(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:03d}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 500, 58])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x24c002eb4f0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "multihead_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "Q = torch.rand(batch_size, seq_len, d_model)\n",
    "K = torch.rand(batch_size, seq_len, d_model)\n",
    "V = torch.rand(batch_size, seq_len, d_model)\n",
    "output, attention = multihead_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_dot_product = ScaledDotProduct()\n",
    "Q = torch.randn(10, 20, 30)\n",
    "K = torch.randn(10, 20, 30)\n",
    "V = torch.randn(10, 20, 40)\n",
    "\n",
    "\n",
    "output, attention = scaled_dot_product(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 40]), torch.Size([20, 20]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, attention[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0163, 0.0329, 0.0278, 0.1355, 0.0135, 0.0550, 0.0034, 0.0812, 0.0678,\n",
       "        0.0717, 0.0605, 0.0011, 0.0840, 0.1410, 0.0687, 0.0337, 0.0112, 0.0352,\n",
       "        0.0548, 0.0048])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder = Decoder(data[0][0].shape[1], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        ...,\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [ 25.0800,  44.9700, -16.8000,  ...,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 25])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decoder.forward(data[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model class\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_len, device, src_pad_idx):\n",
    "        \"\"\"\n",
    "        constructor of TransformerModel class\n",
    "\n",
    "        :param input_size: input size\n",
    "        :param d_model: dimension of model\n",
    "        :param nhead: number of head\n",
    "        :param num_encoder_layers: number of encoder layer\n",
    "        :param dim_feedforward: dimension of feedforward\n",
    "        :param max_len: max sequence length\n",
    "        :param device: hardware device setting\n",
    "        :param src_pad_idx: the index of the source padding token\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.embedding = torch.nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len, device)\n",
    "\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        encoder_norm = torch.nn.LayerNorm(d_model)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        self.fc = torch.nn.Linear(d_model, 1)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        forward method\n",
    "\n",
    "        :param src: source input\n",
    "        :return: output\n",
    "        \"\"\"\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        src = self.embedding(src)\n",
    "        src = src + self.pos_encoder(src)\n",
    "\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask.squeeze())\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "           1.0366e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "           2.0733e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [-8.9797e-01, -4.4006e-01,  4.2620e-01,  ...,  9.9427e-01,\n",
       "           1.0317e-01,  9.9466e-01],\n",
       "         [-8.5547e-01,  5.1785e-01,  9.8628e-01,  ...,  9.9425e-01,\n",
       "           1.0327e-01,  9.9465e-01],\n",
       "         [-2.6461e-02,  9.9965e-01,  6.9756e-01,  ...,  9.9424e-01,\n",
       "           1.0337e-01,  9.9464e-01]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d_model = 512\n",
    "max_len = 1000\n",
    "pe = PositonalEncoding(d_model, max_len)\n",
    "x = torch.zeros((1, max_len, d_model))\n",
    "output1 = pe.forward(x)\n",
    "output1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
